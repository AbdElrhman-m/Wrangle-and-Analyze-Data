{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Twitter Data wangling steps:\n",
    "\n",
    "\n",
    "## 1- Gathering Data\n",
    "---\n",
    "\n",
    "### (A) importing data from csv file:\n",
    "    I have used panads to read the csv file (\"twitter-archive-enhanced.csv\")\n",
    "    \n",
    "### (B)  getting data from udacity server:\n",
    "    I have used request libarary to get the data then used pandas read_csv to read the tsv file (image_predictions.tsv).\n",
    "### (c) Getting data from twitter API\n",
    "    by registering in twitter as a developer I used my twitter developer IDs\n",
    "    I got more info about dog tweets so I saved them in tweets_data.csv\n",
    "    \n",
    "    -------------------------------------------------\n",
    "\n",
    "## 2- Accessing Data\n",
    "\n",
    "> for each data set:\n",
    ">I have assessed it visually and programmatically\n",
    ">\n",
    "> by using:\n",
    "> pandas bluidin function info(), head(), tail(). sample(), isnull() ...etc.\n",
    ">\n",
    "> and I has been writing the problems I see.\n",
    "\n",
    "## 3-  Cleaning Data\n",
    "> I used the 3 steps technique to clean the data (define ==> code ==> clean)\n",
    "> for each step in data cleaning.\n",
    "\n",
    "### the step of data cleaning.\n",
    "\n",
    "###  (A) Quality issues clean\n",
    "> 1-  making a copy of the 3 dataframes \n",
    ">\n",
    "> 2- removing the unnecessary columns for my analysis\n",
    ">\n",
    "> 3- the source column has 3 urls and it will be nicer and cleaner to make a word for each segmentation\n",
    ">\n",
    "> 4- fix data types of the ids to make it easy to merge the tables\n",
    ">\n",
    "> 5- fix the ids to make the archive_clean timestamp datatype to be the datetime\n",
    ">\n",
    "> 6- tweets_clean data have nulls and we have to remove them\n",
    "so we'll drop all the nulls from or dataset\n",
    ">\n",
    "> 7-  in tweets_clean (from the API) we need to change the data type of retweet_countfavorite_count and followers_count to be int.\n",
    ">\n",
    "> 8- fixing the name column in twitter_clean as there some name is just on the small letter, so I'll replace them by an empty string.\n",
    ">\n",
    "> 9- tweets_clean data have nulls and we have to remove them so we'll drop all the nulls from or dataset\n",
    ">\n",
    "> 10-  in tweets_clean (from the API) we need to change the data type of retweet_countfavorite_count and followers_count to be int\n",
    ">\n",
    "> 11-  column p1 ,p2, p3 have names start with lowercase and uppercase so we have to make everything lower case\n",
    ">\n",
    "> 12- rename the jpg_url to img_link\n",
    "\n",
    "### (B) Tidiness issues clean\n",
    ">\n",
    "> 1- making the rating_numerator and rating_denominator to one rating column in archive_clean \n",
    "then remove the two columns \n",
    ">\n",
    "> 2- making the doggo, floofer, pupper & puppo to one dog_stage column in archive_clean \n",
    "then remove the 4 columns.\n",
    ">\n",
    "> 3- in the images_clean dataset, I have picked from the 3 Ps one according to the highest confident, then removing columns.\n",
    "\n",
    "> then I have saved them in the `twitter_archive_master.csv` using pandas.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
